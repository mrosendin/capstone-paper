\documentclass[11pt, oneside]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
% \usepackage{apacite}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
\usepackage[T1]{fontenc}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage[english]{babel}
\usepackage{sectsty}
\usepackage{url, lipsum}

\geometry{letterpaper}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

%-------------------------------------------------------------------------------
% HEADER & FOOTER
%-------------------------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\setlength\headheight{15pt}
\fancyhead[L]{Student ID: 3032162875}
\fancyhead[R]{University of California, Berkeley}
\fancyfoot[R]{Page \thepage\ of \pageref{LastPage}}

%-------------------------------------------------------------------------------
% TITLE PAGE
%-------------------------------------------------------------------------------
\title{
  \normalsize \textsc{    Time Series Forecasting of Monthly Active Users with Long Short-Term Memory
      Networks} \\ [2.0cm]
  \HRule{0.5pt} \\
  \LARGE \textbf{\uppercase{
    Technical Contributions [DRAFT]
  }}
  \HRule{2pt} \\ [0.5cm]
  \vspace*{5\baselineskip}
}
\author{
  Matthew Louis Rosendin \\
  University of California, Berkeley \\
  Department of Industrial Engineering and Operations Research
}
\date{April 13, 2018}

%-------------------------------------------------------------------------------
% DOCUMENT
%-------------------------------------------------------------------------------

\begin{document}
\maketitle
\newpage

\tableofcontents
\newpage

\listoffigures
\newpage

\section{Introduction}
Our project, in partnership with Adobe Systems Inc., is an interactive dashboard that forecasts monthly active users for any Adobe software product. We've accomplished forecasts with excellent accuracy as far as 5 months into the future. The growth model's forecast can be used for a number of reasons, including as an input to forecasting revenue or as a measure of product health. While forecasting monthly active users is nothing new, our application of artificial neural networks (ANNs) seems to be a novel approach that has yielded promising results.

In this chapter of the paper I analyze and discuss the implications of our model's performance and describe my work in deploying our model. I begin by explaining the concrete goals of this work, then outlining the motivation, before finally diving into a technical introduction to our model. Following the basic conceptual discussion around our model, I start the discussion about how I refined the "raw" model code into a flexible, scalable, and configurable object. The translation of the model into a manageable machine learning software system led to additional improvements to the model, such as hyperparameter optimization. A byproduct of the optimization was an exhaustive experimentation of model performance metrics. The underpinning of the preceding work is the user interface. I discuss my work in front-end and systems engineering that resulted in the final deliverable: a forecasting dashboard powered by machine learning.

\subsection{Goal}
Our team's foremost goal is to create a practical and interactive product growth model for Adobe senior management. My individual goal was to design and create a user interface to interact with the model. Throughout my contributions, I've added additional value around testing the performance of our model and gathering relevant metrics.

\subsection{Motivation}
The motivation for creating a dashboard is straightforward. The dashboard enables the user to train the machine learning model resulting in performance metrics and the monthly active user forecast.
The motivation for the section on hyperparameter optimization was simply to improve the results of our model. From the resulting tests, we've found that our model's forecasting capability had improved. Part of the discussion in this paper is reserved for analyzing the source of model improvement.

\subsection{Time Series Forecasting}

In order to explain why we chose our particular model (a long short-term memory network) I will explain the characteristics of the problem we are solving through a brief taxonomy. The problem is best framed as, "how do we forecast monthly active usage multiple weeks into the future?". The most salient characteristic of this problem is that the data is a long sequence (i.e., data is chronologically ordered) of values, known as a \textit{time series}. What we are trying to achieve is a forecast of time series data, so the solution to the problem is known as \textit{time series forecasting}.

\subsection{Autoregressive Models}

The simplest model might be an \textit{autoregressive} (AR) model in which the forecast values are regressed on prior data:

\begin{equation}
X_t = c + \sum_{i=1}^p \phi_i X_{t-i} + \epsilon_t
\end{equation}

where $c$ is a constant, $\epsilon_t$ is an error term, and $\phi_1, ..., \phi_p$ are the parameters to be estimated by linear regression. To forecast a value for the present time $t$, we specify a parameter $p$, known as the "order" or "lag". With $p=3$ the corresponding model would look like this:

\begin{equation}
X_t = c + \phi_1 X_{t-1} + \phi_2 X_{t-2} + \phi_3 X_{t-3} + \epsilon_t
\end{equation}

We would call this an autoregressive model with a lag of 3 time steps where \textit{time steps} are defined as the number of sequential data points. The autoregressive model can be notated as $AR(3)$ to indicate its order with $p=3$.

In our work we use the concept of "lag" from the autoregressive model in our supervised learning model. Our dataset is multivariate, meaning there are more than two observations for each time step. Since we would like to forecast multiple weeks into the future, our model is multi-stepped.

\subsection{Recurrent Neural Networks}

There are many good reasons to justify using an artificial neural network. Much of the recent literature on time series forecasting is focusing on the advantage of ANNs versus other methods, including autoregressive interactive moving average (ARIMA) models. For instance, Ahmed et al. and Zhang show that ANNs are shown to be superior for generalized time series problems \cite{ahmed et al.}. Although complex models such as neural networks can hard to interpret, our project team values performance over interpretability. Furthermore, we have designed features similar to those in financial time series models. For financial stock data, Adebiye et al. show that ANNs are superior to ARIMA models \cite{adebiye et al.}. Lastly, ANNs are capable of modeling non-linearities which can improve the forecast.

\begin{figure}[h]
  \caption{Diagram of a one-unit Long Short-Term Memory (LSTM) network}
  \centering
  \includegraphics[width=12.5cm]{images/Long_Short-Term_Memory.png}
\end{figure}

Recurrent neural networks (RNNs) are a class of artificial neural network where connections between units form a directed graph along a sequence. In other words, RNNs retain state at one time to the next, using the previous state's output for the current estimate. This characteristic enables multi-stepped time series forecasting. A particular architecture of RNNs known as Long Short-Term Memory (LSTM) allows the model to recognize and retain short-term patterns for long periods of time. This architecture is used best where data from an earlier state needs to be recalled at a later state. Examples of LSTM networks in industry include Google Voice \cite{beaufays}.

\section{Object-Oriented Abstraction}
The working LSTM model was created in a Jupyter Notebook. While serving as a minimum proof of concept, the model should not require the end-user engage directly with code. Furthermore, the following criteria were set: 1) the lifecycle of the model (from training, testing, to final predictions) needed to be automated and callable with one command; 2) the Jupyter Notebook code (which executes similarly to a script) needed to be organized into an object-oriented structure to store data within the object instance; 3) the model needed to be capable of loading datasets of arbitrary products. Criterion (2) is more a matter of preference and style to the author, as other programming paradigms could be considered. The rest of this section briefly discusses the process involved in restructuring the model code to meet the criteria.

\subsection{Decomposition}
The goal of decomposition is to break a complex system into parts that are easier to understand, program, and maintain. The process of decomposing the Jupyter Notebook code involved defining separate modules for loading datasets (\texttt{data.py}), feature engineering (\texttt{features.py}), training/testing/predicting (\texttt{core.py}), and accessing utilities (\texttt{utils.py}). Furthermore, the code was organized into functions, and eventually into methods (discussed below).

\subsection{Encapsulation}
After breaking the code into manageable chunks, I created a Python class wrapper to store internal state and to allow the end-user to specify runtime options. The entire model can be run by using the wrapper's simple application programming interface (API).

\section{Hyperparameter Optimization}
\begin{itemize}
  \item{Learning paramters (hyperparameters) vs. model parameters (learned parameters or the weights for features in regression)}
  \item{Runtime and compute resources}
\end{itemize}

\subsection{Cross-validation}
Train/test split time series data samples observed at fixed time intervals, in train/test sets.
No shuffling. Successive splits must have higher test indices.
In the kth split, it returns first k folds as train set and the (k+1)th fold as test set.
Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them.
\subsection{Grid Search}
This search exhaustively generates candidates from a grid of parameter values specified with the \texttt{param\_grid} parameter
\subsection{Results}
\subsection{Discussion}

\section{Systems Engineering}
\subsection{Server Infrastructure}
All software written for this project was developed and tested on local machines. System specifications of the host machine producing the results in this report are shown in Figure~\ref{fig:System_Specs}. The system runs Python 3.6.4 and we've tested on local machines (macOS) and a Docker virtual host container running Ubuntu 16.04. The Docker virtual host container can scale to run multiple instances on a cloud computing platform without any modification to the source code.

\begin{figure}[h]
  \caption{Host machine system information}
  \centering
  \includegraphics[width=12.5cm]{images/System_Specs.png}
  \label{fig:System_Specs}
\end{figure}

\subsection{Distributed Task Queue}
The application can be deployed onto HTTP-accessible hosts where multiple clients can
\section{User Interface}
\subsection{Forecast Visualization}
\begin{figure}[h]
  \caption{User Interface: Dashboard}
  \centering
  \fbox{\includegraphics[width=12.5cm]{images/Dashboard.png}}
\end{figure}

\begin{figure}[h]
  \caption{Product 1 Trend: 6.24\% MAPE}
  \centering
  \includegraphics[width=12.5cm]{images/Product_1-Trend-6_24_Percent_MAPE.png}
  \label{fig:Product_1-Trend}
\end{figure}

Figure~\ref{fig:Product_1-Trend} shows the historical monthly active user trend of Product 1 with the 12 week forecast in the dashed line.

\begin{figure}[h]
  \caption{Product 1 Forecast: 6.24\% MAPE}
  \centering
  \includegraphics[width=12.5cm]{images/Product_1-Forecast-6_24_Percent_MAPE.png}
  \label{fig:Product_1-Forecast}
\end{figure}

Figure~\ref{fig:Product_1-Forecast} shows the forecast for Product 1 over a 12 week period (December 3rd - February 25th 2018).

\subsection{Documentation}
Wrote documentation (akin to a usage guide)

\section{Next Steps}
\subsection{Websockets}
\subsection{Data Pipeline Automation (Apache Hadoop)}
\subsection{Dimensionality Reduction by Feature Selection}
\subsection{Improving Hyperparameter Optimization Efficiency}
\subsection{Confidence Interval Visualization}
A major upgrade would be a visualization of forecast confidence intervals on the current graph.

\subsection{Distributed Machine Learning Clusters}
As discussed previously, the Docker container can be scaled to run on a distributed system.

\section{Conclusion}
In conclusion, ...

\begin{thebibliography}{9}
  % Time series forecasting using a hybrid ARIMA and neural network model
  % https://www.sciencedirect.com/science/article/pii/S0925231201007020

  % Comparison of ARIMA and Artificial Neural Networks Models for Stock Price Prediction
  % https://www.hindawi.com/journals/jam/2014/614342/

  % Hyperparameter optimization - Wikipedia
  % https://en.wikipedia.org/wiki/Hyperparameter_optimization

  % Bayesian information criterion - Wikipedia
  % https://en.wikipedia.org/wiki/Bayesian_information_criterion

  % Building Intelligent Systems: A Guide to Machine Learning Engineering
  % https://books.google.com/books?id=s91PDwAAQBAJ

  % Scaling up Machine Learning
  % https://books.google.com/books?id=9u0gAwAAQBAJ

  % Hidden Technical Debt in Machine Learning Systems
  % https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf

  % Diagram of a one-unit Long Short-Term Memory (LSTM) network
  % https://commons.wikimedia.org/w/index.php?curid=60149410

  \bibitem{adebiye et al.}
  Adebiyi, A. A., Adewumi, A. O., \& Ayo, C. K. (2014). Comparison of ARIMA and Artificial Neural Network Models for Stock Price Prediction. \textit{Journal of Applied Mathematics}, \textit{2014}.

  \bibitem{ahmed et al.}
  Ahmed, N. K., Atiya, A. F., Gayar, N. E., \& El-Shishiny, H. (2010). An Empirical Comparison of Machine Learning Models for Time Series Forecasting. \textit{Econometric Reviews}, \textit{29}(5-6).

  \bibitem{beaufays}
  Beaufays. (2015, August 11). The neural networks behind Google Voice transcription [Blog post]. Retrieved from https://research.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html.

  % Google voice search: faster and more accurate
  % https://research.googleblog.com/2015/09/google-voice-search-faster-and-more.html
\end{thebibliography}

\end{document}
